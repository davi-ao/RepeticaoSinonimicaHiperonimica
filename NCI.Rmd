---
title: "Network Construction-Integration"
output:
  pdf_document:
    toc: yes
    toc_depth: '5'
    latex_engine: xelatex
  html_notebook:
    toc: yes
    toc_depth: 5
    number_sections: yes
bibliography: references.bib
csl: instituto-brasileiro-de-informacao-em-ciencia-e-tecnologia-abnt-initials.csl
---

# Introduction

The method described and implemented in this file comprises the steps for implementing a reading model based on the Construction-Integration (CI) framework \[@Kintsch1998\] using the programming language and environment R [@R]. The present method expands the CI framework with the implementation of network analysis of texts in the construction of semantic representations, substituting the use of manually constructed propositions.

```{r include=FALSE}
# Load the necessary packages
# It may be necessary to install these packages first
library(tidyverse)
library(udpipe)
library(wordnet)
library(igraph)
library(ggraph)

# Donwload and load the english-ewt and portuguese-br models for udpipe package
# model_ewt = udpipe_download_model(language = 'english-ewt')
# model_por = udpipe_download_model(language = 'portuguese-br')

m_eng = udpipe_load_model('udpipe-models/english-ewt-ud-2.5-191206.udpipe')
m_por = udpipe_load_model('udpipe-models/portuguese-br-ud-2.0-170801.udpipe')

# Configure wordnet dictionary for English
setDict('wn3.1.dict/dict/')

```

## PT-BR resources

```{r}
# Load pt-br lemmas
lemmas_por = read_file('lemmatization-pt.txt') %>%
  str_split('\\n') %>%
  unlist() %>%
  tibble(lemmas = .) %>%
  separate(lemmas, into = c('lemma', 'flexao'), sep = '\\t') %>%
  filter(!is.na(flexao)) %>%
  mutate(lemma = lemma %>% str_remove_all('\\r'),
         flexao = flexao %>% str_remove_all('\\r')) %>% 
  rename(token = flexao)

# Load pt-br synonyms
synonyms_por = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
  str_split('\n') %>%
  unlist() %>%
  tibble(sinonimo = .) %>%
  separate(sinonimo, c('token', 'upos', 'synonym'), sep = ' ') %>%
  filter(!is.na(synonym)) %>%
  mutate(upos = upos %>% str_match('_(.+)_') %>% .[,2]) %>%
  mutate(upos = upos %>% recode(N = 'NOUN', V = 'VERB'))

# Load pt-br hypernyms
hypernyms_por = read_file('PAPEL/relacoes_final_HIPERONIMIA.txt') %>%
  str_split('\n') %>%
  unlist() %>%
  tibble(hiperonimo = .) %>%
  separate(hiperonimo, c('token', 'relation', 'hypernym'), sep = ' ') %>%
  filter(!is.na(hypernym)) %>%
  select(token, hypernym)
```

# Functions

## Annotate text

```{r}
# Annotate texts with udpipe
annotate_text = function(text, language = 'EN') {
  model = m_eng
  
  if (language == 'PT') {
    model = m_por
  }
  
  annotated_text = text %>% 
    udpipe_annotate(model, .) %>%
    as_tibble()
  
  if (language == 'PT') {
    annotated_text = annotated_text %>%
      select(-lemma) %>%
      mutate(token = token %>% 
               str_to_lower()) %>%
      left_join(lemmas_por, by = 'token')
  }
  
  annotated_text %>% 
    select(sentence_id, lemma, upos) %>%
    filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
    mutate(lemma = lemma %>% str_to_lower()) %>%
    group_by(sentence_id) %>%
    distinct() %>%
    ungroup()
}
```

## Get synonyms

```{r}
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
  if (language == 'EN') {
    synonyms_list = mapply(function(w, p) {
      if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
        return()
      } else {
        pos_f = p
        
        if (p == 'ADJ') {
          pos_f = 'ADJECTIVE'
        } else if (p == 'ADV') {
          pos_f = 'ADVERB'
        }
        
        synonyms(w, pos_f)
      }
    }, words, POS)
    
    synonyms_tibble = tibble(lemma = names(synonyms_list),
                             synonyms = synonyms_list %>%
                               sapply(function(s) {
                                 paste(s,
                                       sep = '',
                                       collapse = ',')})) %>%
      filter(synonyms != '') %>%
      separate_rows(synonyms, sep = ',') %>%
      filter(!str_detect(synonyms, '\\s')) %>%
      mutate(synonyms = synonyms %>% 
               str_remove_all('\\(a\\)|\\(p\\)') %>%
               str_to_lower()) %>%
    filter(lemma != synonyms) %>%
    distinct()
    
    synonyms_tibble
  } else if (language == 'PT') {
    synonyms_tibble = tibble(token = words, upos = POS) %>%
      left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
      select(token, synonym) %>%
      filter(!is.na(synonym)) %>%
      rename(lemma = token, synonyms = synonym) %>%
      distinct()
    
    synonyms_tibble
  }
}
```

## Get hypernyms

```{r}
# Get hypernyms
get_words_hypernyms = function(words, POS, language = 'EN') {
  if (language == 'EN') {
    hypernyms_list = mapply(function(w, p) {
      if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
        return()
      } else {
        pos_f = p
        
        if (p == 'ADJ') {
          pos_f = 'ADJECTIVE'
        } else if (p == 'ADV') {
          pos_f = 'ADVERB'
        }
        
        tryCatch({
          filter = getTermFilter('ExactMatchFilter', w, T)
          terms = getIndexTerms(pos_f, 1, filter)
          synsets = getSynsets(terms[[1]])
          
          sapply(synsets, function(s) {
            relatedSynsets = getRelatedSynsets(s, '@')
            
            sapply(relatedSynsets, getWord)
          })
        }, error = function(e) {
          message(e)
          message('\n')
        })
      }
  }, words, POS)
  
  hypernyms_tibble = tibble(lemma = names(hypernyms_list),
                            hypernyms = hypernyms_list %>%
                              sapply(function(l) {
                                sapply(l, function(h) {
                                  paste(h, sep = '', collapse = ',')
                                }) %>%
                                  paste(sep = '', collapse = ',')})) %>%
    filter(hypernyms != '') %>%
    separate_rows(hypernyms, sep = ',') %>%
    filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
    mutate(hypernyms = hypernyms %>%
             str_remove_all('c\\(|"')) %>%
    filter(lemma != hypernyms) %>%
    distinct()
  
    hypernyms_tibble
  } else if (language == 'PT') {
    hypernyms_tibble = tibble(token = words) %>%
      left_join(hypernyms_por, by = 'token') %>%
      select(token, hypernym) %>%
      filter(!is.na(hypernym)) %>%
      rename(lemma = token, hypernyms = hypernym) %>%
      distinct()
    
    hypernyms_tibble
  }
}
```

## Save network in Pajek format

```{r}
# Save network as Pajek .net file
save_net = function(edges_list, filename) {
  vertices_tbl = edges_list %>% 
    .$Source %>% 
    unique() %>% 
    tibble(vertices = .) %>% 
    mutate(id = row_number())
  vertices_firstline = paste('*Vertices', vertices_tbl %>% nrow())
  vertices = paste0(1:(vertices_tbl %>% nrow()), 
                    ' "', 
                    vertices_tbl$vertices, 
                    '"')
  edges_firstline = '*Edges'
  edges = paste(edges_list %>% 
                           rename(vertices = Source) %>% 
                           left_join(vertices_tbl, by='vertices') %>%
                           .$id,
                         edges_list %>% 
                           rename(vertices = Target) %>% 
                           left_join(vertices_tbl, by='vertices') %>%
                           .$id)
  
  if (file.exists(filename)) {
    file.remove(filename)
  }
  
  write(vertices_firstline, filename)
  
  for (i in 1:length(vertices)) {
    write(vertices[i], filename, append=T)
  }
  
  write(edges_firstline, filename, append = T)
  
  for (i in 1:length(edges)) {
    write(edges[i], filename, append = T)
  }
}
```

## Create network of cliques

```{r}
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
  # Get relevant synonyms and hypernyms
  lemmas_in_cycle = text_lexical %>%
    filter(sentence_id %in% 1:cycle) %>%
    .$lemma %>% unique()
  
  semantic_relations = synonyms %>%
    filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
      bind_rows(hypernyms %>%
                  filter(lemma %in% lemmas_in_cycle & 
                           hypernyms %in% lemmas_in_cycle)) %>%
      pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
      filter(!is.na(Target)) %>%
      rename(Source = lemma) %>%
      select(-type)
  
  text_lexical %>%
    filter(sentence_id %in% 1:cycle) %>%
    rename(Source = lemma) %>%
    mutate(Target = Source) %>%
    group_by(sentence_id) %>%
    expand(Source, Target) %>%
    filter(Source != Target) %>%
    ungroup() %>%
    select(-sentence_id) %>%
    bind_rows(semantic_relations) %>%
    save_net(., file)
  
  read_graph(file, format = 'pajek') %>% 
    simplify()
}
```

```{r}
# Spreak activation
spread_activation = function(current, previous) {
  if (is.na(previous) %>% all()) {
    A = rep(1, length(V(current)))
    W = as_adjacency_matrix(current)
    
    repeat {
      A = (A %*% W)/max(A %*% W)
      
      if (any(abs(A - (A %*% W)/max(A %*% W)) < .001)) {
        break
      }
    }
    
    final_activation = A
    
    final_activation[1,]
  } else {
    A = c(V(previous)$activation, rep(1, length(V(current)) - length(V(previous))))
    W = as_adjacency_matrix(current)
    
    final_activation = (A %*% W)/max(A %*% W)
    
    final_activation[1,]
  }
}
```

```{r}
# Plot network
plot_net = function(net) {
  net %>%
    ggraph(layout = 'kk') +
    geom_edge_link(color = "#eeeeee", edge_width = .5) +
    geom_node_point(aes(size = activation, alpha = activation), color = "#555555") +
    geom_node_text(aes(label = name)) +
    theme_void() + 
    scale_size(range = c(2, 10))
}
```

# Procedure

## Load texts

```{r}
# Load the Englishh text
text = read_file('NCI_text1.txt')

# Load the Portuguese texts
news1 = read_file('noticia1.txt')
news2 = read_file('noticia2.txt')
news3 = read_file('noticia3.txt')
```

## Tokenization, lemmatization and POS tagging

```{r}
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)

news1_annotated = annotate_text(news1, 'PT')
news2_annotated = annotate_text(news2, 'PT')
news3_annotated = annotate_text(news3, 'PT')
```

## Remove grammatical words

```{r}
text_lexical = text_annotated %>%
  filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))

news1_lexical = news1_annotated %>%
  filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))

news2_lexical = news2_annotated %>%
  filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))

news3_lexical = news3_annotated %>%
  filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
```

## Get synonyms and hypernyms

```{r}
# Get synonyms tibbles
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
                                  text_lexical$upos)

news1_synonyms_tibble = get_words_synonyms(news1_lexical$lemma,
                                           news1_lexical$upos,
                                           'PT')

news2_synonyms_tibble = get_words_synonyms(news2_lexical$lemma,
                                           news2_lexical$upos,
                                           'PT')

news3_synonyms_tibble = get_words_synonyms(news3_lexical$lemma,
                                           news3_lexical$upos,
                                           'PT')

# Get hypernyms tibbles
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
                                       text_lexical$upos)

news1_hypernyms_tibble = get_words_hypernyms(news1_lexical$lemma,
                                           news1_lexical$upos,
                                           'PT')

news2_hypernyms_tibble = get_words_hypernyms(news2_lexical$lemma,
                                           news2_lexical$upos,
                                           'PT')

news3_hypernyms_tibble = get_words_hypernyms(news3_lexical$lemma,
                                           news3_lexical$upos,
                                           'PT')

# TODO Get hyponyms tibbles
```

## Search synonymic and hyperonymic relations

```{r}
# Direct synonyms
direct_synonyms_eng = synonyms_tibble %>%
  filter(synonyms %in% text_lexical$lemma)

direct_synonyms1_por = news1_synonyms_tibble %>%
  filter(synonyms %in% news1_lexical$lemma)

direct_synonyms2_por = news2_synonyms_tibble %>%
  filter(synonyms %in% news2_lexical$lemma)

direct_synonyms3_por = news3_synonyms_tibble %>%
  filter(synonyms %in% news3_lexical$lemma)

# Direct hypernyms
direct_hypernyms_eng = hypernyms_tibble %>%
  filter(hypernyms %in% text_lexical$lemma)

direct_hypernyms1_por = news1_hypernyms_tibble %>%
  filter(hypernyms %in% news1_lexical$lemma)

direct_hypernyms2_por = news2_hypernyms_tibble %>%
  filter(hypernyms %in% news2_lexical$lemma)

direct_hypernyms3_por = news3_hypernyms_tibble %>%
  filter(hypernyms %in% news3_lexical$lemma)
```

## Build Networks

\[First cycle\]

```{r}
# Create clique network
cycle1 = create_net(text_lexical, 
                    1, 
                    'redes_cycles/cycle1.net', 
                    direct_synonyms, 
                    direct_hypernyms)
news1_cycle1 = create_net(news1_lexical, 
                         1, 
                         'redes_news/news1_c1.net',
                         direct_synonyms1_por,
                         direct_hypernyms1_por)
# ERROR
news2_cycle = create_net(news2_lexical, 
                         1, 
                         'redes_news/news2_c1.net',
                         direct_synonyms2_por,
                         direct_hypernyms2_por)

# ERROR
news3_cycle = create_net(news3_lexical, 
                         1, 
                         'redes_news/news3_c1.net',
                         direct_synonyms3_por,
                         direct_hypernyms3_por)

# Spread activation
V(cycle1)$activation = spread_activation(cycle1, NA)
V(news1_cycle1)$activation = spread_activation(news1_cycle1, NA)


cycle1 %>% 
  plot_net()

news1_cycle1 %>%
  plot_net()
```

\[Second cycle\]

```{r}
# Create clique network
cycle2 = create_net(text_lexical, 2)
news1_cycle2 = create_net(news1_lexical, 
                         2, 
                         'redes_news/news1_c2.net',
                         direct_synonyms1_por,
                         direct_hypernyms1_por)

# Spread activation
V(cycle2)$activation = spread_activation(cycle2, cycle1)
V(news1_cycle2)$activation = spread_activation(news1_cycle2, news1_cycle1)

cycle2 %>% 
  plot_net()

news1_cycle2 %>%
  plot_net()
```

\[Third cycle\]

```{r}
# Create clique network
cycle3 = create_net(text_lexical, 3)
news1_cycle3 = create_net(news1_lexical, 
                         3, 
                         'redes_news/news1_c3.net',
                         direct_synonyms1_por,
                         direct_hypernyms1_por)

# Spread activation
V(cycle3)$activation = spread_activation(cycle3, cycle2)
V(news1_cycle3)$activation = spread_activation(news1_cycle3, news1_cycle2)

cycle3 %>% 
  plot_net()

news1_cycle3 %>%
  plot_net()
```

\[Fourth Cycle\]

```{r}
# Create clique network
cycle4 = create_net(text_lexical, 4)

# Spread activation
V(cycle4)$activation = spread_activation(cycle4, cycle3)

cycle4 %>% 
  plot_net()
```

\[Cycles 5 to 34\]

```{r}
remaining_cycles = lapply(5:34, function(c) {
  create_net(text_lexical, c)
})

for (c in 1:length(remaining_cycles)) {
  if (c == 1) {
    V(remaining_cycles[[c]])$activation = spread_activation(
      remaining_cycles[[c]], cycle4)
  } else {
    V(remaining_cycles[[c]])$activation = spread_activation(
      remaining_cycles[[c]], remaining_cycles[[c - 1]])
  }
}

remaining_cycles[[30]] %>% 
  plot_net() %>%
  ggsave('last_cycle.png', plot = ., device = 'png', width = 297, height = 210, units = 'mm')
```

\[Comparação com difusão de ativação somente na rede final\]

```{r}
final_network = create_net(text_lexical, 
                           34,
                           'redes/final.net',
                           synonyms_tibble,
                           hypernyms_tibble)

V(final_network)$activation = spread_activation(final_network, NA)

final_network %>%
  plot_net()
```

\[Métricas das redes\]

```{r}
# Transitivity of final network
remaining_cycles[[30]] %>% transitivity() # Spreading activation per cycle
final_network %>% transitivity() # Spreading activation only in final network

# Transitivity of random counterpart
length(E(remaining_cycles[[30]])) / (length(V(remaining_cycles[[30]])) * (length(V(remaining_cycles[[30]])) - 1))

# Degree distribution
remaining_cycles[[30]] %>% # Spreading activation per cycle
  degree() %>%
  tibble(k = .) %>%
  group_by(k) %>%
  count() %>%
  ungroup() %>%
  mutate(p_k = n/sum(n)) %>%
  ggplot(aes(k, p_k)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()

final_network %>% # Spreading activation per cycle
  degree() %>%
  tibble(k = .) %>%
  group_by(k) %>%
  count() %>%
  ungroup() %>%
  mutate(p_k = n/sum(n)) %>%
  ggplot(aes(k, p_k)) +
  geom_point()
```

\[Correlation with most used words on recall - lm with frequency of recalled words\]

```{r}
recall = read_file('NCI_recall.txt') %>%
  udpipe_annotate(m_eng, .) %>%
  as_tibble() %>%
  select(sentence_id, lemma, upos) %>%
  filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
  mutate(lemma = lemma %>% str_to_lower()) %>%
  group_by(sentence_id) %>%
  distinct() %>%
  ungroup()

# Activation considering spreading per cycle
activation_recall = tibble(lemma = V(remaining_cycles[[30]])$name, 
       activation = V(remaining_cycles[[30]])$activation) %>%
  left_join(recall %>%
              filter(!upos %in% 
                       c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
              group_by(lemma) %>%
              count(),
            by = 'lemma')

activation_recall %>%
  filter(!is.na(n)) %>%
  ggplot(aes(activation, n)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = 'lm')

# Activation considering spreading in the final network
activation_recall = tibble(lemma = V(final_network)$name, 
       activation = V(final_network)$activation) %>%
  left_join(recall %>%
              filter(!upos %in% 
                       c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
              group_by(lemma) %>%
              count(),
            by = 'lemma')

activation_recall %>%
  filter(!is.na(n)) %>%
  ggplot(aes(activation, n)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(method = 'lm')
```

\[Linear model\]

```{r}
# Simple linear model
activation_recall %>%
  lm(n ~ activation, data = .) %>%
  summary()
```

\[glm with probability of text word being recalled\]

```{r}
recall_participant = read_csv('recall_participants.csv')

# Activation considering spreading per cycle
activation_recall = recall_participant %>%
  left_join(tibble(lemma = V(remaining_cycles[[30]])$name, 
       activation = V(remaining_cycles[[30]])$activation),
       by = 'lemma') %>%
  filter(!is.na(activation))

activation_recall %>%
  group_by(lemma) %>%
  summarize(p = mean(recall), activation = unique(activation)) %>%
  ggplot(aes(activation, p)) +
  geom_point() + 
  scale_y_log10() +
  scale_x_log10()

# Activation considering spreading in the final network
activation_recall = recall_participant %>%
  left_join(tibble(lemma = V(final_network)$name, 
       activation = V(final_network)$activation),
       by = 'lemma') %>%
  filter(!is.na(activation))

activation_recall %>%
  group_by(lemma) %>%
  summarize(p = mean(recall), activation = unique(activation)) %>%
  ggplot(aes(activation, p)) +
  geom_point() +
  scale_y_log10() + 
  scale_x_log10()
```

```{r}
library(lme4)
library(lmerTest)
library(sjPlot)

# Logistic multilevel model
model_logistic = activation_recall %>%
  mutate(recall = ifelse(recall, 1, 0) %>% factor()) %>%
  glmer(recall ~ activation + (1|participant), data = ., family = binomial())

tab_model(model_logistic, transform = NULL)
plot_model(model_logistic, type = 'pred', terms = 'activation[all]', show.data = T)
```

```{r}
# Redes de notícias
news_1_net = read_graph('redes_news/news_1.net', format = 'pajek') %>% 
    simplify()

V(news_1_net)$activation = spread_activation(news_1_net, NA)

news_1_net %>%
  plot_net()
```

# References
