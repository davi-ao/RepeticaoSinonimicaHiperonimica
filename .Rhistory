View(news2_synonyms_tibble)
View(news2_hypernyms_tibble)
# Direct synonyms
direct_synonyms_eng = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
direct_synonyms1_por = news1_synonyms_tibble %>%
filter(synonyms %in% news1_lexical$lemma)
direct_synonyms2_por = news2_synonyms_tibble %>%
filter(synonyms %in% news2_lexical$lemma)
direct_synonyms3_por = news3_synonyms_tibble %>%
filter(synonyms %in% news3_lexical$lemma)
# Direct hypernyms
direct_hypernyms_eng = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
direct_hypernyms1_por = news1_hypernyms_tibble %>%
filter(hypernyms %in% news1_lexical$lemma)
direct_hypernyms2_por = news2_hypernyms_tibble %>%
filter(hypernyms %in% news2_lexical$lemma)
direct_hypernyms3_por = news3_hypernyms_tibble %>%
filter(hypernyms %in% news3_lexical$lemma)
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., file)
read_graph(file, format = 'pajek') %>%
simplify()
}
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
library(tidyverse)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
recall_participant = read_csv('recall_participants.csv')
# Activation considering spreading per cycle
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation),
by = 'lemma') %>%
filter(!is.na(activation))
library(igraph)
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
final_network = create_net(text_lexical, 34)
final_network = create_net(text_lexical, 34, synonyms_tibble)
final_network = create_net(text_lexical,
34,
'redes/final.net'
synonyms_tibble,
final_network = create_net(text_lexical,
34,
'redes/final.net',
synonyms_tibble,
hypernyms_tibble)
V(final_network)$activation = spread_activation(final_network, NA)
final_network %>%
plot_net()
library(ggraph)
final_network %>%
plot_net()
recall = read_file('NCI_recall.txt') %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
# Degree distribution
remaining_cycles[[30]] %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, cumsum(p_k))) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
library(pdftools)
# Carregar pacotes necessÃ¡rios
library(tidyverse)
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths[1]
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
folder = './ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
article_text = pdf_text(paste0(folder, paths[1]))
article_text
write_file(article_text, 'corpus/article001.txt')
article_text %>% length()
article_text %>% str_c()
article_text %>% str_c(collapse = '')
article_text %>% str_c(collapse = '') %>% length()
article_text[1]
article_text[2]
article_text[3]
write_file(article_text %>% str_c(collapse = ''), 'corpus/article001.txt')
write_file(article_text %>% str_c(collapse = ''),
'Artigo1/corpus/article001.txt')
str_pad('1', 3, pad = '0')
str_pad('2', 3, pad = '0')
str_pad('10', 3, pad = '0')
str_pad('100', 3, pad = '0')
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(1), 3, pad = '0'),
'.txt'))
length(paths)
for (i in i:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
for (i in 1:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
# Load the necessary packages
# It may be necessary to install these packages first
library(tidyverse)
library(udpipe)
library(wordnet)
library(igraph)
library(ggraph)
# Donwload and load the english-ewt and portuguese-br models for udpipe package
# model_ewt = udpipe_download_model(language = 'english-ewt')
# model_por = udpipe_download_model(language = 'portuguese-br')
m_eng = udpipe_load_model('udpipe-models/english-ewt-ud-2.5-191206.udpipe')
m_por = udpipe_load_model('udpipe-models/portuguese-br-ud-2.0-170801.udpipe')
# Configure wordnet dictionary for English
setDict('wn3.1.dict/dict/')
# Annotate texts with udpipe
annotate_text = function(text, language = 'EN') {
model = m_eng
if (language == 'PT') {
model = m_por
}
annotated_text = text %>%
udpipe_annotate(model, .) %>%
as_tibble()
if (language == 'PT') {
annotated_text = annotated_text %>%
select(-lemma) %>%
mutate(token = token %>%
str_to_lower()) %>%
left_join(lemmas_por, by = 'token')
}
annotated_text %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
}
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
synonyms_tibble
} else if (language == 'PT') {
synonyms_tibble = tibble(token = words, upos = POS) %>%
left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
select(token, synonym) %>%
filter(!is.na(synonym)) %>%
rename(lemma = token, synonyms = synonym) %>%
distinct()
synonyms_tibble
}
}
# Get hypernyms
get_words_hypernyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
hypernyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
tryCatch({
filter = getTermFilter('ExactMatchFilter', w, T)
terms = getIndexTerms(pos_f, 1, filter)
synsets = getSynsets(terms[[1]])
sapply(synsets, function(s) {
relatedSynsets = getRelatedSynsets(s, '@')
sapply(relatedSynsets, getWord)
})
}, error = function(e) {
message(e)
message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
} else if (language == 'PT') {
hypernyms_tibble = tibble(token = words) %>%
left_join(hypernyms_por, by = 'token') %>%
select(token, hypernym) %>%
filter(!is.na(hypernym)) %>%
rename(lemma = token, hypernyms = hypernym) %>%
distinct()
hypernyms_tibble
}
}
# Save network as Pajek .net file
save_net = function(edges_list, filename) {
vertices_tbl = edges_list %>%
.$Source %>%
unique() %>%
tibble(vertices = .) %>%
mutate(id = row_number())
vertices_firstline = paste('*Vertices', vertices_tbl %>% nrow())
vertices = paste0(1:(vertices_tbl %>% nrow()),
' "',
vertices_tbl$vertices,
'"')
edges_firstline = '*Edges'
edges = paste(edges_list %>%
rename(vertices = Source) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id,
edges_list %>%
rename(vertices = Target) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id)
if (file.exists(filename)) {
file.remove(filename)
}
write(vertices_firstline, filename)
for (i in 1:length(vertices)) {
write(vertices[i], filename, append=T)
}
write(edges_firstline, filename, append = T)
for (i in 1:length(edges)) {
write(edges[i], filename, append = T)
}
}
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., file)
read_graph(file, format = 'pajek') %>%
simplify()
}
# Spreak activation
spread_activation = function(current, previous) {
if (is.na(previous) %>% all()) {
A = rep(1, length(V(current)))
W = as_adjacency_matrix(current)
repeat {
A = (A %*% W)/max(A %*% W)
if (any(abs(A - (A %*% W)/max(A %*% W)) < .001)) {
break
}
}
final_activation = A
final_activation[1,]
} else {
A = c(V(previous)$activation, rep(1, length(V(current)) - length(V(previous))))
W = as_adjacency_matrix(current)
final_activation = (A %*% W)/max(A %*% W)
final_activation[1,]
}
}
# Plot network
plot_net = function(net) {
net %>%
ggraph(layout = 'kk') +
geom_edge_link(color = "#eeeeee", edge_width = .5) +
geom_node_point(aes(size = activation, alpha = activation), color = "#555555") +
geom_node_text(aes(label = name)) +
theme_void() +
scale_size(range = c(2, 10))
}
# Load the Englishh text
text = read_file('NCI_text1.txt')
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)
text_lexical = text_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
# Get synonyms tibbles
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
# Get synonyms tibbles
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos,
'EN')
?read_graph
final_network = read_graph('redes/final.net', format = 'pajek')
V(final_network)$activation = spread_activation(final_network, NA)
final_network %>%
plot_net()
V(final_network)
E(final_network)
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_bar(stat = 'identity')
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
