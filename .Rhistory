filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
}
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)
news1_annotated = annotate_text(news1, 'PT')
View(news1_annotated)
news2_annotated = annotate_text(news2, 'PT')
news3_annotated = annotate_text(news3, 'PT')
news1_lexical = news1_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
news2_lexical = news2_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
news3_lexical = news3_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
View(news1_lexical)
# Load pt-br synonyms
sinonimos = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('sinonimo1', 'classe', 'sinonimo2'), sep = ' ') %>%
filter(!is.na(sinonimo2)) %>%
mutate(classe = classe %>% str_match('_(.+)_') %>% .[,2])
View(sinonimos)
# Load pt-br synonyms
synonyms_por = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('token', 'upos', 'synonym'), sep = ' ') %>%
filter(!is.na(synonym)) %>%
mutate(upos = upos %>% str_match('_(.+)_') %>% .[,2])
View(synonyms_por)
synonyms_por$upos %>% unique()
synonyms_por %>% recode(upos, N = 'NOUN', V = 'VERB')
synonyms_por$upos %>% recode(N = 'NOUN', V = 'VERB')
# Load pt-br synonyms
synonyms_por = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('token', 'upos', 'synonym'), sep = ' ') %>%
filter(!is.na(synonym)) %>%
mutate(upos = upos %>% str_match('_(.+)_') %>% .[,2]) %>%
mutate(upos = upos %>% recode(N = 'NOUN', V = 'VERB'))
# Get synonyms tibble
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
text_lexical = text_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
# Get synonyms tibble
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
# Get hypernyms tibble
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
View(synonyms_tibble)
View(synonyms_por)
# Load the necessary packages
# It may be necessary to install these packages first
library(tidyverse)
library(udpipe)
library(wordnet)
library(igraph)
library(ggraph)
# Donwload and load the english-ewt and portuguese-br models for udpipe package
# model_ewt = udpipe_download_model(language = 'english-ewt')
# model_por = udpipe_download_model(language = 'portuguese-br')
m_eng = udpipe_load_model('udpipe-models/english-ewt-ud-2.5-191206.udpipe')
m_por = udpipe_load_model('udpipe-models/portuguese-br-ud-2.0-170801.udpipe')
# Configure wordnet dictionary for English
setDict('wn3.1.dict/dict/')
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
View(direct_synonyms)
View(synonyms_tibble)
View(synonyms_por)
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
synonyms_tibble
} else if (language == 'PT') {
synonyms_tibble = tibble(token = words, upos = POS) %>%
left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
select(token, synonym) %>%
rename(lemma = token, synonyms = synonym)
synonyms_tibble
}
}
news1_synonyms_tibble = get_words_synonyms(news1_lexical$lemma,
news1_lexical$upos,
'PT')
View(news1_synonyms_tibble)
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
synonyms_tibble
} else if (language == 'PT') {
synonyms_tibble = tibble(token = words, upos = POS) %>%
left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
select(token, synonym) %>%
filter(!is.na(synonym)) %>%
rename(lemma = token, synonyms = synonym)
synonyms_tibble
}
}
news2_synonyms_tibble = get_words_synonyms(news2_lexical$lemma,
news2_lexical$upos,
'PT')
news3_synonyms_tibble = get_words_synonyms(news3_lexical$lemma,
news3_lexical$upos,
'PT')
View(news1_synonyms_tibble)
View(news1_synonyms_tibble)
View(news2_synonyms_tibble)
# Load pt-br hypernyms
hypernyms_por = read_file('PAPEL/relacoes_final_HIPERONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(hiperonimo = .) %>%
separate(hiperonimo, c('token', 'upos', 'hypernym'), sep = ' ') %>%
filter(!is.na(hypernym))
View(hypernyms_por)
# Load pt-br hypernyms
hypernyms_por = read_file('PAPEL/relacoes_final_HIPERONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(hiperonimo = .) %>%
separate(hiperonimo, c('token', 'relation', 'hypernym'), sep = ' ') %>%
filter(!is.na(hypernym)) %>%
select(token, hypernym)
text_lexical$lemma
c(text_lexical$lemma, news1_lexical$lemma)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% c(text_lexical$lemma,
news1_lexical$lemma,
news2_lexical$lemma,
news3_lexical$lemma))
# Get hypernyms
get_words_hypernyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
hypernyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
tryCatch({
filter = getTermFilter('ExactMatchFilter', w, T)
terms = getIndexTerms(pos_f, 1, filter)
synsets = getSynsets(terms[[1]])
sapply(synsets, function(s) {
relatedSynsets = getRelatedSynsets(s, '@')
sapply(relatedSynsets, getWord)
})
}, error = function(e) {
message(e)
message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
} else if (language == 'PT') {
hypernyms_tibble = tibble(token = words) %>%
left_join(hypernyms_por, by = 'token') %>%
select(token, hypernym) %>%
filter(!is.na(hypernym)) %>%
rename(lemma = token, hypernyms = hypernym)
hypernyms_tibble
}
}
# Get hypernyms tibbles
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
news1_hypernyms_tibble = get_words_hypernyms(news1_lexical$lemma,
news1_lexical$upos,
'PT')
news2_hypernyms_tibble = get_words_hypernyms(news2_lexical$lemma,
news2_lexical$upos,
'PT')
news3_hypernyms_tibble = get_words_hypernyms(news3_lexical$lemma,
news3_lexical$upos,
'PT')
View(news1_hypernyms_tibble)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% c(text_lexical$lemma,
news1_lexical$lemma,
news2_lexical$lemma,
news3_lexical$lemma))
View(direct_hypernyms)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
library(tidyverse)
library(igraph)
View(news2_lexical)
View(news2_synonyms_tibble)
View(news2_hypernyms_tibble)
# Direct synonyms
direct_synonyms_eng = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
direct_synonyms1_por = news1_synonyms_tibble %>%
filter(synonyms %in% news1_lexical$lemma)
direct_synonyms2_por = news2_synonyms_tibble %>%
filter(synonyms %in% news2_lexical$lemma)
direct_synonyms3_por = news3_synonyms_tibble %>%
filter(synonyms %in% news3_lexical$lemma)
# Direct hypernyms
direct_hypernyms_eng = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
direct_hypernyms1_por = news1_hypernyms_tibble %>%
filter(hypernyms %in% news1_lexical$lemma)
direct_hypernyms2_por = news2_hypernyms_tibble %>%
filter(hypernyms %in% news2_lexical$lemma)
direct_hypernyms3_por = news3_hypernyms_tibble %>%
filter(hypernyms %in% news3_lexical$lemma)
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., file)
read_graph(file, format = 'pajek') %>%
simplify()
}
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
library(tidyverse)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
recall_participant = read_csv('recall_participants.csv')
# Activation considering spreading per cycle
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation),
by = 'lemma') %>%
filter(!is.na(activation))
library(igraph)
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
final_network = create_net(text_lexical, 34)
final_network = create_net(text_lexical, 34, synonyms_tibble)
final_network = create_net(text_lexical,
34,
'redes/final.net'
synonyms_tibble,
final_network = create_net(text_lexical,
34,
'redes/final.net',
synonyms_tibble,
hypernyms_tibble)
V(final_network)$activation = spread_activation(final_network, NA)
final_network %>%
plot_net()
library(ggraph)
final_network %>%
plot_net()
recall = read_file('NCI_recall.txt') %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
# Degree distribution
remaining_cycles[[30]] %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, cumsum(p_k))) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
library(pdftools)
# Carregar pacotes necessários
library(tidyverse)
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths[1]
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
folder = './ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
article_text = pdf_text(paste0(folder, paths[1]))
article_text
write_file(article_text, 'corpus/article001.txt')
article_text %>% length()
article_text %>% str_c()
article_text %>% str_c(collapse = '')
article_text %>% str_c(collapse = '') %>% length()
article_text[1]
article_text[2]
article_text[3]
write_file(article_text %>% str_c(collapse = ''), 'corpus/article001.txt')
write_file(article_text %>% str_c(collapse = ''),
'Artigo1/corpus/article001.txt')
str_pad('1', 3, pad = '0')
str_pad('2', 3, pad = '0')
str_pad('10', 3, pad = '0')
str_pad('100', 3, pad = '0')
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(1), 3, pad = '0'),
'.txt'))
length(paths)
for (i in i:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
for (i in 1:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
