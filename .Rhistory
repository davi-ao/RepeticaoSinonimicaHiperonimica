V(remaining_cycles[[30]])$name
tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation)
library(pdftools)
pdf_text('thesis.pdf')
recall = read_file('NCI_recall.txt') %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
recall
recall %>%
group_by(lemma) %>%
count()
recall %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
group_by(lemma) %>%
count()
tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation) %>%
left_join(recall %>%
filter(!upos %in%
c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
group_by(lemma) %>%
count(),
by = 'lemma')
activation_recall = tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation) %>%
left_join(recall %>%
filter(!upos %in%
c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
group_by(lemma) %>%
count(),
by = 'lemma')
View(activation_recall)
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(activation, n) +
geom_point()
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point()
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10() +
geom_smooth(method = 'lm')
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
geom_smooth(method = 'lm')
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
geom_smooth()
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10() +
geom_smooth()
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10() +
geom_smooth(method = 'lm')
activation_recall %>%
lm(n ~ activation, data = .) %>%
summary()
# Load the necessary packages
# It may be necessary to install these packages first
library(tidyverse)
library(udpipe)
library(wordnet)
library(igraph)
library(ggraph)
m_eng = udpipe_load_model(model_ewt$file_model)
# Configure wordnet dictionary
setDict('wn3.1.dict/dict/')
# Define helper functions
get_words_synonyms = function(words, POS) {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
}
get_words_hypernyms = function(words, POS) {
hypernyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
tryCatch({
filter = getTermFilter('ExactMatchFilter', w, T)
terms = getIndexTerms(pos_f, 1, filter)
synsets = getSynsets(terms[[1]])
sapply(synsets, function(s) {
relatedSynsets = getRelatedSynsets(s, '@')
sapply(relatedSynsets, getWord)
})
}, error = function(e) {
message(e)
message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
}
save_net = function(edges_list, filename) {
vertices_tbl = edges_list %>%
.$Source %>%
unique() %>%
tibble(vertices = .) %>%
mutate(id = row_number())
vertices_firstline = paste('*Vertices', vertices_tbl %>% nrow())
vertices = paste0(1:(vertices_tbl %>% nrow()),
' "',
vertices_tbl$vertices,
'"')
edges_firstline = '*Edges'
edges = paste(edges_list %>%
rename(vertices = Source) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id,
edges_list %>%
rename(vertices = Target) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id)
if (file.exists(filename)) {
file.remove(filename)
}
write(vertices_firstline, filename)
for (i in 1:length(vertices)) {
write(vertices[i], filename, append=T)
}
write(edges_firstline, filename, append = T)
for (i in 1:length(edges)) {
write(edges[i], filename, append = T)
}
}
create_net = function(text_lexical, cycle) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = direct_synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(direct_hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., paste0('cycle', cycle, '.net'))
read_graph(paste0('cycle', cycle, '.net'), format = 'pajek') %>%
simplify()
}
spread_activation = function(current, previous) {
if (is.na(previous) %>% all()) {
A = rep(1, length(V(current)))
W = as_adjacency_matrix(current)
final_activation = (A %*% W)/max(A %*% W)
final_activation[1,]
} else {
A = c(V(previous)$activation, rep(1, length(V(current)) - length(V(previous))))
W = as_adjacency_matrix(current)
final_activation = (A %*% W)/max(A %*% W)
final_activation[1,]
}
}
spread_activation = function(current, previous) {
if (is.na(previous) %>% all()) {
A = rep(1, length(V(current)))
W = as_adjacency_matrix(current)
repeat {
A = (A %*% W)/max(A %*% W)
if (any(abs(A - (A %*% W)/max(A %*% W)) < .001)) {
break
}
}
final_activation = A
final_activation[1,]
} else {
A = c(V(previous)$activation, rep(1, length(V(current)) - length(V(previous))))
W = as_adjacency_matrix(current)
final_activation = (A %*% W)/max(A %*% W)
final_activation[1,]
}
}
plot_net = function(net) {
net %>%
ggraph(layout = 'kk') +
geom_edge_link(color = "#eeeeee", edge_width = .5) +
geom_node_point(aes(size = activation, alpha = activation), color = "#555555") +
geom_node_text(aes(label = name)) +
theme_void() +
scale_size(range = c(2, 10))
}
# Load the text
text = read_file('NCI_text1.txt')
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = text %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
text_lexical = text_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
# Get synonyms tibble
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
# Get hypernyms tibble
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
# Create clique network
cycle1 = create_net(text_lexical, 1)
# Spread activation
V(cycle1)$activation = spread_activation(cycle1, NA)
cycle1 %>%
plot_net()
# Create clique network
cycle2 = create_net(text_lexical, 2)
# Spread activation
V(cycle2)$activation = spread_activation(cycle2, cycle1)
cycle2 %>%
plot_net()
# Create clique network
cycle3 = create_net(text_lexical, 3)
# Spread activation
V(cycle3)$activation = spread_activation(cycle3, cycle2)
cycle3 %>%
plot_net()
# Create clique network
cycle4 = create_net(text_lexical, 4)
# Spread activation
V(cycle4)$activation = spread_activation(cycle4, cycle3)
cycle4 %>%
plot_net()
remaining_cycles = lapply(5:34, function(c) {
create_net(text_lexical, c)
})
for (c in 1:length(remaining_cycles)) {
if (c == 1) {
V(remaining_cycles[[c]])$activation = spread_activation(
remaining_cycles[[c]], cycle4)
} else {
V(remaining_cycles[[c]])$activation = spread_activation(
remaining_cycles[[c]], remaining_cycles[[c - 1]])
}
}
remaining_cycles[[30]] %>%
plot_net()
library(pdftools)
recall = read_file('NCI_recall.txt') %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
activation_recall = tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation) %>%
left_join(recall %>%
filter(!upos %in%
c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ')) %>%
group_by(lemma) %>%
count(),
by = 'lemma')
activation_recall %>%
filter(!is.na(n)) %>%
ggplot(aes(activation, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10() +
geom_smooth(method = 'lm')
activation_recall %>%
lm(n ~ activation, data = .) %>%
summary()
library(tidyverse)
library(igraph)
remaining_cycles[[30]] %>% transitivity()
# Transitivity of random counterpart
length(E(remaining_cycles[[30]]))
# Transitivity of random counterpart
length(E(remaining_cycles[[30]])) / length(V(remaining_cycles[[30]])) * (length(V(remaining_cycles[[30]])) - 1)
# Transitivity of random counterpart
length(E(remaining_cycles[[30]])) / (length(V(remaining_cycles[[30]])) * (length(V(remaining_cycles[[30]])) - 1))
# Transitivity of final network
remaining_cycles[[30]] %>% transitivity()
# Transitivity of random counterpart
length(E(remaining_cycles[[30]])) / (length(V(remaining_cycles[[30]])) * (length(V(remaining_cycles[[30]])) - 1))
# Degree distribution
remaining_cycles[[30]] %>% degree_distribution()
# Degree distribution
remaining_cycles[[30]] %>%
degree_distribution() %>%
ggplot(aes(.)) +
geom_point()
# Degree distribution
remaining_cycles[[30]] %>%
degree_distribution() %>%
ggplot(aes(.)) +
geom_histogram()
# Degree distribution
remaining_cycles[[30]] %>%
degree_distribution() %>%
hist()
# Degree distribution
remaining_cycles[[30]] %>%
degree_distribution() %>%
tibble(k = .)
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .)
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count()
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ggplot(aes(k, n))
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ggplot(aes(k, n)) +
geom_point()
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ggplot(aes(k, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ggplot(aes(k, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10() +
geom_smooth(method = 'lm')
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ggplot(aes(k, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count()
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
mutate(p_k = n/sum(n))
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n))
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(p_k, n)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
# Degree distribution
remaining_cycles[[30]] %>%
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
remaining_cycles[[30]] %>%
plot_net() %>%
ggsave('last_cycle.png', width=1920, height = 1080, units = 'px', device = 'png')
library(tidyverse)
remaining_cycles[[30]] %>%
plot_net() %>%
ggsave('last_cycle.png', plot = ., device = 'png', width = 1920, height = 1080, units = 'px')
remaining_cycles[[30]] %>%
plot_net() %>%
ggsave('last_cycle.png', plot = ., device = 'png', width = 1920, height = 1080, units = 'px')
library(ggraph)
remaining_cycles[[30]] %>%
plot_net() %>%
ggsave('last_cycle.png', plot = ., device = 'png', width = 1920, height = 1080, units = 'px')
remaining_cycles[[30]] %>%
plot_net() %>%
ggsave('last_cycle.png', plot = ., device = 'png', width = 297, height = 210, units = 'mm')
