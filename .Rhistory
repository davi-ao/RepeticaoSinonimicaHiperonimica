} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
tryCatch({
filter = getTermFilter('ExactMatchFilter', w, T)
terms = getIndexTerms(pos_f, 1, filter)
synsets = getSynsets(terms[[1]])
sapply(synsets, function(s) {
relatedSynsets = getRelatedSynsets(s, '@')
sapply(relatedSynsets, getWord)
})
}, error = function(e) {
message(e)
message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
} else if (language == 'PT') {
hypernyms_tibble = tibble(token = words) %>%
left_join(hypernyms_por, by = 'token') %>%
select(token, hypernym) %>%
filter(!is.na(hypernym)) %>%
rename(lemma = token, hypernyms = hypernym)
hypernyms_tibble
}
}
# Get hypernyms tibbles
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
news1_hypernyms_tibble = get_words_hypernyms(news1_lexical$lemma,
news1_lexical$upos,
'PT')
news2_hypernyms_tibble = get_words_hypernyms(news2_lexical$lemma,
news2_lexical$upos,
'PT')
news3_hypernyms_tibble = get_words_hypernyms(news3_lexical$lemma,
news3_lexical$upos,
'PT')
View(news1_hypernyms_tibble)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% c(text_lexical$lemma,
news1_lexical$lemma,
news2_lexical$lemma,
news3_lexical$lemma))
View(direct_hypernyms)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
library(tidyverse)
library(igraph)
View(news2_lexical)
View(news2_synonyms_tibble)
View(news2_hypernyms_tibble)
# Direct synonyms
direct_synonyms_eng = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
direct_synonyms1_por = news1_synonyms_tibble %>%
filter(synonyms %in% news1_lexical$lemma)
direct_synonyms2_por = news2_synonyms_tibble %>%
filter(synonyms %in% news2_lexical$lemma)
direct_synonyms3_por = news3_synonyms_tibble %>%
filter(synonyms %in% news3_lexical$lemma)
# Direct hypernyms
direct_hypernyms_eng = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
direct_hypernyms1_por = news1_hypernyms_tibble %>%
filter(hypernyms %in% news1_lexical$lemma)
direct_hypernyms2_por = news2_hypernyms_tibble %>%
filter(hypernyms %in% news2_lexical$lemma)
direct_hypernyms3_por = news3_hypernyms_tibble %>%
filter(hypernyms %in% news3_lexical$lemma)
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., file)
read_graph(file, format = 'pajek') %>%
simplify()
}
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
library(tidyverse)
activation_recall %>%
group_by(lemma) %>%
summarize(p = mean(recall), activation = unique(activation)) %>%
ggplot(aes(activation, p)) +
geom_point() +
scale_y_log10() +
scale_x_log10()
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
recall_participant = read_csv('recall_participants.csv')
# Activation considering spreading per cycle
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(remaining_cycles[[30]])$name,
activation = V(remaining_cycles[[30]])$activation),
by = 'lemma') %>%
filter(!is.na(activation))
library(igraph)
# Activation considering spreading in the final network
activation_recall = recall_participant %>%
left_join(tibble(lemma = V(final_network)$name,
activation = V(final_network)$activation),
by = 'lemma') %>%
filter(!is.na(activation))
final_network = create_net(text_lexical, 34)
final_network = create_net(text_lexical, 34, synonyms_tibble)
final_network = create_net(text_lexical,
34,
'redes/final.net'
synonyms_tibble,
final_network = create_net(text_lexical,
34,
'redes/final.net',
synonyms_tibble,
hypernyms_tibble)
V(final_network)$activation = spread_activation(final_network, NA)
final_network %>%
plot_net()
library(ggraph)
final_network %>%
plot_net()
recall = read_file('NCI_recall.txt') %>%
udpipe_annotate(m_eng, .) %>%
as_tibble() %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
# Degree distribution
remaining_cycles[[30]] %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, cumsum(p_k))) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point() +
scale_x_log10() +
scale_y_log10()
final_network %>% # Spreading activation per cycle
degree() %>%
tibble(k = .) %>%
group_by(k) %>%
count() %>%
ungroup() %>%
mutate(p_k = n/sum(n)) %>%
ggplot(aes(k, p_k)) +
geom_point()
library(pdftools)
# Carregar pacotes necessÃ¡rios
library(tidyverse)
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths = list.files('Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/')
paths[1]
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
folder = './ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
folder = 'Artigo1/ScienceDirect_articles_10Mar2022_12-19-53.718/'
paths = list.files(folder)
article_text = pdf_text(paste0(folder, paths[1]))
article_text
write_file(article_text, 'corpus/article001.txt')
article_text %>% length()
article_text %>% str_c()
article_text %>% str_c(collapse = '')
article_text %>% str_c(collapse = '') %>% length()
article_text[1]
article_text[2]
article_text[3]
write_file(article_text %>% str_c(collapse = ''), 'corpus/article001.txt')
write_file(article_text %>% str_c(collapse = ''),
'Artigo1/corpus/article001.txt')
str_pad('1', 3, pad = '0')
str_pad('2', 3, pad = '0')
str_pad('10', 3, pad = '0')
str_pad('100', 3, pad = '0')
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(1), 3, pad = '0'),
'.txt'))
length(paths)
for (i in i:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
for (i in 1:length(paths)) {
article_text = pdf_text(paste0(folder, paths[i]))
write_file(article_text %>% str_c(collapse = ''),
paste0('Artigo1/corpus/article',
str_pad(toString(i), 3, pad = '0'),
'.txt'))
}
article = GET('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5')
library(httr)
article = GET('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5')
content(article)
library(jsonlite)
content(article) %>%
read_json()
library(tidyverse)
library(httr)
library(jsonlite)
article = GET('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5')
content(article) %>%
read_json()
content(article)
content(article) %>%
as_json()
content(article) %>%
View()
fromJSON('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5')
library(tidyjson)
install.packages("tidyjson")
library(tidyjson)
fromJSON('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5') %>%
spread_all()
response = GET('https://api.elsevier.com/content/article/pii/S0950705122000727?httpAccept=application/json&apiKey=d31a652c065492a05a4eba1d94a683a5')
content(response) %>%
spread_all()
content(response) %>%
select(originalText)
content(response) %>%
gather_object(originalText)
content(response) %>%
gather_object('originalText')
content(response) %>%
gather_object()
content(response) %>%
gather_object() %>%
gather_keys('originalText')
content(response) %>%
gather_object() %>%
gather_object('originalText')
content(response) %>%
gather_object() %>%
spread(originalText)
content(response) %>%
gather_object() %>%
spread('originalText')
content(response) %>%
gather_object() %>%
spread_values()
content(response) %>%
gather_object() %>%
spread_all()
content(response) %>%
gather_object() %>%
spread_all()
content(response) %>%
gather_object()
content(response) %>%
gather_object() %>%
.$originalData
content(response) %>%
gather_object()
content(response) %>%
spread_all()
content(response) %>%
spread_all() %>%
select(originalText)
content(response) %>%
spread_all() %>%
select(originalText) %>%
class()
content(response) %>%
spread_all() %>%
select(originalText) %>%
as_tibble()
library(udpipe)
m_eng = udpipe_load_model('udpipe-models/english-ewt-ud-2.5-191206.udpipe')
content(response) %>%
spread_all() %>%
select(originalText) %>%
as_tibble() %>%
.$originalText
text = content(response) %>%
spread_all() %>%
select(originalText) %>%
as_tibble() %>%
.$originalText
udpipe_annotate(m_eng, text)
udpipe_annotate(m_eng, text) %>%
as_tibble() %>%
View()
library(tidyverse)
library(rvest)
html = read_html('Artigo1/body.html')
html
html %>%
html_elements('p')
html %>%
html_elements('p') %>%
html_attrs('id')
html %>%
html_elements('p') %>%
html_attrs()
html %>%
html_elements('p') %>%
html_attr('id')
html %>%
html_elements('p') %>%
html_text2()
tibble(id = html %>%
html_elements('p') %>%
html_attr('id'),
text = html %>%
html_elements('p') %>%
html_text2())
tibble(id = html %>%
html_elements('p') %>%
html_attr('id'),
text = html %>%
html_elements('p') %>%
html_text2()) %>% View()
filter(str_detect('^p\\d{4}$')) %>%
View()
filter(id %>% str_detect('^p\\d{4}$')) %>%
View()
filter(str_detect(id, '^p\\d{4}$')) %>%
View()
filter(str_detect(.$id, '^p\\d{4}$')) %>%
View()
filter(str_detect(id, '^p\\d{4}$')) %>%
View()
tibble(id = html %>%
html_elements('p') %>%
html_attr('id'),
text = html %>%
html_elements('p') %>%
html_text2()) %>%
filter(str_detect(id, '^p\\d{4}$')) %>%
View()
tibble(id = html %>%
html_elements('p') %>%
html_attr('id'),
text = html %>%
html_elements('p') %>%
html_text2()) %>%
filter(str_detect(id, '^p\\d{4}$')) %>%
.$text
corpus_info = read_file('corpus.txt')
corpus_info = read_file('corpus.txt') %>%
str_split('\\n\\n')
View(corpus_info)
read_file('corpus.txt')
read_file('corpus.txt') %>%
str_split('\\n\\n')
corpus_info = read_file('corpus.txt') %>%
str_split('\n\n')
View(corpus_info)
corpus_info = read_file('corpus.txt') %>%
str_split('\n')
corpus_info = read_file('corpus.txt') %>%
str_split('\\n')
corpus_info = read_file('corpus.txt') %>%
str_split('\\n', simplify = T)
corpus_info
View(corpus_info)
corpus_info = read_file('corpus.txt') %>%
str_split('\\n\\n', simplify = T)
corpus_info = read_file('corpus.txt') %>%
str_split('\\n{2}', simplify = T)
corpus_info = read_file('corpus.txt') %>%
tibble(text = .)
View(corpus_info)
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n\n')
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n')
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = regex('\n\n', multiline = T))
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = regex('\\n\\n', multiline = T))
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = regex('\n{2}', multiline = T))
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n')
1:nrow(corpus_info)
1:nrow(corpus_info)/12
1:(nrow(corpus_info)/12)
1:(nrow(corpus_info)/12) * 12
1:(nrow(corpus_info)/12) * 12 + 1
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n')
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n') %>%
filter(nrow() %in% 1:(nrow(corpus_info)/12) * 12 + 1)
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n') %>%
filter(row_number() %in% 1:(nrow(corpus_info)/12) * 12 + 1)
(1:(nrow(corpus_info)/12) * 12 + 1)
corpus_info = read_file('corpus.txt') %>%
tibble(text = .) %>%
separate_rows(text, sep = '\n') %>%
filter(row_number() %in% (1:(nrow(corpus_info)/12) * 12 + 1))
