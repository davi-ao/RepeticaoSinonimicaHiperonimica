message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
} else if (language == 'PT') {
}
}
# Save network as Pajek .net file
save_net = function(edges_list, filename) {
vertices_tbl = edges_list %>%
.$Source %>%
unique() %>%
tibble(vertices = .) %>%
mutate(id = row_number())
vertices_firstline = paste('*Vertices', vertices_tbl %>% nrow())
vertices = paste0(1:(vertices_tbl %>% nrow()),
' "',
vertices_tbl$vertices,
'"')
edges_firstline = '*Edges'
edges = paste(edges_list %>%
rename(vertices = Source) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id,
edges_list %>%
rename(vertices = Target) %>%
left_join(vertices_tbl, by='vertices') %>%
.$id)
if (file.exists(filename)) {
file.remove(filename)
}
write(vertices_firstline, filename)
for (i in 1:length(vertices)) {
write(vertices[i], filename, append=T)
}
write(edges_firstline, filename, append = T)
for (i in 1:length(edges)) {
write(edges[i], filename, append = T)
}
}
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = direct_synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(direct_hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., paste0('cycle', cycle, '.net'))
read_graph(paste0('cycle', cycle, '.net'), format = 'pajek') %>%
simplify()
}
# Spreak activation
spread_activation = function(current, previous) {
if (is.na(previous) %>% all()) {
A = rep(1, length(V(current)))
W = as_adjacency_matrix(current)
repeat {
A = (A %*% W)/max(A %*% W)
if (any(abs(A - (A %*% W)/max(A %*% W)) < .001)) {
break
}
}
final_activation = A
final_activation[1,]
} else {
A = c(V(previous)$activation, rep(1, length(V(current)) - length(V(previous))))
W = as_adjacency_matrix(current)
final_activation = (A %*% W)/max(A %*% W)
final_activation[1,]
}
}
# Plot network
plot_net = function(net) {
net %>%
ggraph(layout = 'kk') +
geom_edge_link(color = "#eeeeee", edge_width = .5) +
geom_node_point(aes(size = activation, alpha = activation), color = "#555555") +
geom_node_text(aes(label = name)) +
theme_void() +
scale_size(range = c(2, 10))
}
# Load the Englishh text
text = read_file('NCI_text1.txt')
# Load the Portuguese texts
news1 = read_file('noticia1.txt')
news2 = read_file('noticia2.txt')
news3 = read_file('noticia3.txt')
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)
View(text_annotated)
# Annotate texts with udpipe
annotate_text = function(text, language = 'EN') {
model = m_eng
if (language == 'PT') {
model = m_por
}
annotated_text = text %>%
udpipe_annotate(model, .) %>%
as_tibble() %>%
if (language == 'PT') {
annotated_text = annotated_text %>%
select(-lemma) %>%
mutate(token = token %>%
str_to_lower()) %>%
left_join(lemmas_por, by = 'token')
}
annotated_text %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
}
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)
'EN' == 'PT'
'EN' == 'EN'
# Annotate texts with udpipe
annotate_text = function(text, language = 'EN') {
model = m_eng
if (language == 'PT') {
model = m_por
}
annotated_text = text %>%
udpipe_annotate(model, .) %>%
as_tibble()
if (language == 'PT') {
annotated_text = annotated_text %>%
select(-lemma) %>%
mutate(token = token %>%
str_to_lower()) %>%
left_join(lemmas_por, by = 'token')
}
annotated_text %>%
select(sentence_id, lemma, upos) %>%
filter(upos != 'PUNCT' & !str_detect(lemma, '\\W')) %>%
mutate(lemma = lemma %>% str_to_lower()) %>%
group_by(sentence_id) %>%
distinct() %>%
ungroup()
}
# Annotate text with udpipe
# Select lemmas, remove punctuation, remove contractions ('s and 'd), transform to lowercase and remove repeated lemmas
text_annotated = annotate_text(text)
news1_annotated = annotate_text(news1, 'PT')
View(news1_annotated)
news2_annotated = annotate_text(news2, 'PT')
news3_annotated = annotate_text(news3, 'PT')
news1_lexical = news1_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
news2_lexical = news2_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
news3_lexical = news3_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
View(news1_lexical)
# Load pt-br synonyms
sinonimos = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('sinonimo1', 'classe', 'sinonimo2'), sep = ' ') %>%
filter(!is.na(sinonimo2)) %>%
mutate(classe = classe %>% str_match('_(.+)_') %>% .[,2])
View(sinonimos)
# Load pt-br synonyms
synonyms_por = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('token', 'upos', 'synonym'), sep = ' ') %>%
filter(!is.na(synonym)) %>%
mutate(upos = upos %>% str_match('_(.+)_') %>% .[,2])
View(synonyms_por)
synonyms_por$upos %>% unique()
synonyms_por %>% recode(upos, N = 'NOUN', V = 'VERB')
synonyms_por$upos %>% recode(N = 'NOUN', V = 'VERB')
# Load pt-br synonyms
synonyms_por = read_file('PAPEL/relacoes_final_SINONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(sinonimo = .) %>%
separate(sinonimo, c('token', 'upos', 'synonym'), sep = ' ') %>%
filter(!is.na(synonym)) %>%
mutate(upos = upos %>% str_match('_(.+)_') %>% .[,2]) %>%
mutate(upos = upos %>% recode(N = 'NOUN', V = 'VERB'))
# Get synonyms tibble
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
text_lexical = text_annotated %>%
filter(!upos %in% c('ADP', 'AUX', 'CCONJ', 'DET', 'PART', 'SCONJ'))
# Get synonyms tibble
synonyms_tibble = get_words_synonyms(text_lexical$lemma,
text_lexical$upos)
# Get hypernyms tibble
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
View(synonyms_tibble)
View(synonyms_por)
# Load the necessary packages
# It may be necessary to install these packages first
library(tidyverse)
library(udpipe)
library(wordnet)
library(igraph)
library(ggraph)
# Donwload and load the english-ewt and portuguese-br models for udpipe package
# model_ewt = udpipe_download_model(language = 'english-ewt')
# model_por = udpipe_download_model(language = 'portuguese-br')
m_eng = udpipe_load_model('udpipe-models/english-ewt-ud-2.5-191206.udpipe')
m_por = udpipe_load_model('udpipe-models/portuguese-br-ud-2.0-170801.udpipe')
# Configure wordnet dictionary for English
setDict('wn3.1.dict/dict/')
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
View(direct_synonyms)
View(synonyms_tibble)
View(synonyms_por)
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
synonyms_tibble
} else if (language == 'PT') {
synonyms_tibble = tibble(token = words, upos = POS) %>%
left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
select(token, synonym) %>%
rename(lemma = token, synonyms = synonym)
synonyms_tibble
}
}
news1_synonyms_tibble = get_words_synonyms(news1_lexical$lemma,
news1_lexical$upos,
'PT')
View(news1_synonyms_tibble)
# Get synonyms
get_words_synonyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
synonyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
synonyms(w, pos_f)
}
}, words, POS)
synonyms_tibble = tibble(lemma = names(synonyms_list),
synonyms = synonyms_list %>%
sapply(function(s) {
paste(s,
sep = '',
collapse = ',')})) %>%
filter(synonyms != '') %>%
separate_rows(synonyms, sep = ',') %>%
filter(!str_detect(synonyms, '\\s')) %>%
mutate(synonyms = synonyms %>%
str_remove_all('\\(a\\)|\\(p\\)') %>%
str_to_lower()) %>%
filter(lemma != synonyms) %>%
distinct()
synonyms_tibble
} else if (language == 'PT') {
synonyms_tibble = tibble(token = words, upos = POS) %>%
left_join(synonyms_por, by = c("token" = "token", "upos" = "upos")) %>%
select(token, synonym) %>%
filter(!is.na(synonym)) %>%
rename(lemma = token, synonyms = synonym)
synonyms_tibble
}
}
news2_synonyms_tibble = get_words_synonyms(news2_lexical$lemma,
news2_lexical$upos,
'PT')
news3_synonyms_tibble = get_words_synonyms(news3_lexical$lemma,
news3_lexical$upos,
'PT')
View(news1_synonyms_tibble)
View(news1_synonyms_tibble)
View(news2_synonyms_tibble)
# Load pt-br hypernyms
hypernyms_por = read_file('PAPEL/relacoes_final_HIPERONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(hiperonimo = .) %>%
separate(hiperonimo, c('token', 'upos', 'hypernym'), sep = ' ') %>%
filter(!is.na(hypernym))
View(hypernyms_por)
# Load pt-br hypernyms
hypernyms_por = read_file('PAPEL/relacoes_final_HIPERONIMIA.txt') %>%
str_split('\n') %>%
unlist() %>%
tibble(hiperonimo = .) %>%
separate(hiperonimo, c('token', 'relation', 'hypernym'), sep = ' ') %>%
filter(!is.na(hypernym)) %>%
select(token, hypernym)
text_lexical$lemma
c(text_lexical$lemma, news1_lexical$lemma)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% c(text_lexical$lemma,
news1_lexical$lemma,
news2_lexical$lemma,
news3_lexical$lemma))
# Get hypernyms
get_words_hypernyms = function(words, POS, language = 'EN') {
if (language == 'EN') {
hypernyms_list = mapply(function(w, p) {
if (p %in% c('NUM', 'PROPN', 'PRON', 'SYM')) {
return()
} else {
pos_f = p
if (p == 'ADJ') {
pos_f = 'ADJECTIVE'
} else if (p == 'ADV') {
pos_f = 'ADVERB'
}
tryCatch({
filter = getTermFilter('ExactMatchFilter', w, T)
terms = getIndexTerms(pos_f, 1, filter)
synsets = getSynsets(terms[[1]])
sapply(synsets, function(s) {
relatedSynsets = getRelatedSynsets(s, '@')
sapply(relatedSynsets, getWord)
})
}, error = function(e) {
message(e)
message('\n')
})
}
}, words, POS)
hypernyms_tibble = tibble(lemma = names(hypernyms_list),
hypernyms = hypernyms_list %>%
sapply(function(l) {
sapply(l, function(h) {
paste(h, sep = '', collapse = ',')
}) %>%
paste(sep = '', collapse = ',')})) %>%
filter(hypernyms != '') %>%
separate_rows(hypernyms, sep = ',') %>%
filter(!str_detect(hypernyms, '\\s') & hypernyms != '') %>%
mutate(hypernyms = hypernyms %>%
str_remove_all('c\\(|"')) %>%
filter(lemma != hypernyms) %>%
distinct()
hypernyms_tibble
} else if (language == 'PT') {
hypernyms_tibble = tibble(token = words) %>%
left_join(hypernyms_por, by = 'token') %>%
select(token, hypernym) %>%
filter(!is.na(hypernym)) %>%
rename(lemma = token, hypernyms = hypernym)
hypernyms_tibble
}
}
# Get hypernyms tibbles
hypernyms_tibble = get_words_hypernyms(text_lexical$lemma,
text_lexical$upos)
news1_hypernyms_tibble = get_words_hypernyms(news1_lexical$lemma,
news1_lexical$upos,
'PT')
news2_hypernyms_tibble = get_words_hypernyms(news2_lexical$lemma,
news2_lexical$upos,
'PT')
news3_hypernyms_tibble = get_words_hypernyms(news3_lexical$lemma,
news3_lexical$upos,
'PT')
View(news1_hypernyms_tibble)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% c(text_lexical$lemma,
news1_lexical$lemma,
news2_lexical$lemma,
news3_lexical$lemma))
View(direct_hypernyms)
# Direct synonyms
direct_synonyms = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
# Direct hypernyms
direct_hypernyms = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
library(tidyverse)
library(igraph)
View(news2_lexical)
View(news2_synonyms_tibble)
View(news2_hypernyms_tibble)
# Direct synonyms
direct_synonyms_eng = synonyms_tibble %>%
filter(synonyms %in% text_lexical$lemma)
direct_synonyms1_por = news1_synonyms_tibble %>%
filter(synonyms %in% news1_lexical$lemma)
direct_synonyms2_por = news2_synonyms_tibble %>%
filter(synonyms %in% news2_lexical$lemma)
direct_synonyms3_por = news3_synonyms_tibble %>%
filter(synonyms %in% news3_lexical$lemma)
# Direct hypernyms
direct_hypernyms_eng = hypernyms_tibble %>%
filter(hypernyms %in% text_lexical$lemma)
direct_hypernyms1_por = news1_hypernyms_tibble %>%
filter(hypernyms %in% news1_lexical$lemma)
direct_hypernyms2_por = news2_hypernyms_tibble %>%
filter(hypernyms %in% news2_lexical$lemma)
direct_hypernyms3_por = news3_hypernyms_tibble %>%
filter(hypernyms %in% news3_lexical$lemma)
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
# Create network of cliques with additional edges
create_net = function(text_lexical, cycle, file, synonyms, hypernyms) {
# Get relevant synonyms and hypernyms
lemmas_in_cycle = text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
.$lemma %>% unique()
semantic_relations = synonyms %>%
filter(lemma %in% lemmas_in_cycle & synonyms %in% lemmas_in_cycle) %>%
bind_rows(hypernyms %>%
filter(lemma %in% lemmas_in_cycle &
hypernyms %in% lemmas_in_cycle)) %>%
pivot_longer(-lemma, names_to = 'type', values_to = 'Target') %>%
filter(!is.na(Target)) %>%
rename(Source = lemma) %>%
select(-type)
text_lexical %>%
filter(sentence_id %in% 1:cycle) %>%
rename(Source = lemma) %>%
mutate(Target = Source) %>%
group_by(sentence_id) %>%
expand(Source, Target) %>%
filter(Source != Target) %>%
ungroup() %>%
select(-sentence_id) %>%
bind_rows(semantic_relations) %>%
save_net(., file)
read_graph(file, format = 'pajek') %>%
simplify()
}
news1_cycle = create_net(news1_lexical,
1,
'redes_news/news1_c1.net',
direct_synonyms1_por,
direct_hypernyms1_por)
